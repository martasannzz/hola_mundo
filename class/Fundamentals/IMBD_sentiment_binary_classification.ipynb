{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martasannzz/hola_mundo/blob/main/class/Fundamentals/IMBD_sentiment_binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poQhfVhIlSJk"
      },
      "source": [
        "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
        "<table align=\"center\">\n",
        " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/Fundamentals/IMBD_sentiment_binary_classification.ipynb\">\n",
        "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/IMBD_sentiment_binary_classification.ipynb\">\n",
        "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoyTDoNMlSJo"
      },
      "source": [
        "# Classification Example\n",
        " Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n",
        "VAMOS A CONSTRUIR UN ANALIZADOR DE SENTIMIENTO EN TEXTOS CON UNA RED NEURONAL.\n",
        "EN ESTA PRACTICA ES RECOMENDABLE USAR UNA TARJETA GRAFICA DE GOOGLE COLAB:\n",
        "EDIT>NOTEBOOK SETTINGS>GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mjROSDBllSJp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka6rmGPrlSJq"
      },
      "source": [
        "## The Dataset: The IMDB dataset\n",
        "We’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The  parameter `num_words` controls how many words different we want to use.\n",
        "EL CONJUNTO QUE VAMOS A USAR ES UN CONJUNTO DE 50000 REVIEWS DE PELICULAS QUE ESTÁN MUY POLARIZADAS. LA CLASE 0 ES QUE SON NEGATIVAS Y LA CLASE 1 ES QUE SON POSITIVAS. VAMOS A TRATAR DE HACER UN CLASIFICADOR DE SENTIMIENTO PARA PREDECIR CUANDO UN TEXTO ES POSITIVO Y CUANDO UN TEXTO ES NEGATIVO.\n",
        "ESTE DATASET ESTA EN KERAS Y TIENE UNOS CUANTOS DATASETS PARA HACER PRUEBAS.\n",
        "TIENE UN PARÁMETRO QUE ES EL NÚMERO DE PALANRAS Y QUE ES PARA FIJAR EL NÚMERO DE PALABRAS QUE QUEREMOS PROCESAR. ESTO SIRVE PORQUE POR EJEMPLO EN ESTE DATASET QUE TIENE 60000 O 70000 PALABRAS DIFERENTES, ESO ES UN MODELO MUY GRANDE Y HABRÁ PALABRAS QUE SOLO APAREZCAN 1 SOLA VEZ. ENTONCES ASI PONEMOS EL NÚMERO MÁXIMO DE PALABRAS QUE QUEREMOS PROCESAR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVWzC3ivlSJq",
        "outputId": "6568f418-1b0e-409d-bc31-a8a47ff46781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "num_words = 10000\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words) # NOS DIVIDE EL DATASET EN ENTRENAMIENTO Y TEST.\n",
        "# EN TRAIN DATA ESTÁN LAS LISTAS DE TEXTOS Y EN LABELS VA A SER 0 O 1 DEPENDIENDO DE SI ES POSITIVA O NEGATIVA.\n",
        "print(train_data[0])\n",
        "#UN PRIMER EJEMPLO PARA VISUALIZAR. AQUI VEMOS QUE VIENE UNNA SERIE DE INDICES PRIMERO (NO VIENEN LAS PALABRAS, YA ESTÁ PROCESADO. A CADA PALABRA/TOKEN LE ASIGA UN ÍNDICE)\n",
        "# POR EJEMPLO LA PALABA HELLO PUEDE TENER EL INDICE 3, OTRA PALABRA EL 4 Y ASI SUCESIVAMENTE\n",
        "# VAMOS A TENER QUE CREAR UN DICCIONARIO PARA TRANSFORMAR LOS INDICES A PALABRAS Y LAS PALABRAS A LOS INDICES. ESTO LO HACEMOS CON WORD2INT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG562sJ-lSJr"
      },
      "outputs": [],
      "source": [
        "# Transform word_id to word and reverse\n",
        "word2int = imdb.get_word_index()\n",
        "word2int = {w: i+3 for w, i in word2int.items()} # A CADA PALABRA LE DAMOS UN INDICE Y A CADA INDICE LE DAMOS PALABRA\n",
        "\n",
        "# HAY UNA SERIE DE PALABRAS ADICIONALES A NUESTRO VOCABULARIO\n",
        "#. POR EJEMPLO EL PADING ES CADAA VEZ QUE EMPIEZA UNA LISTA Y LUEGO EL CARACTER UNK ES PARA PALABRAS QUE NO ESTAN EN NUESTRO VOCABULARIO. SI HEMOS COGIDO 10000 PALABRAS, LA 15000 NO VA A ESTAR Y ENTONCES LE ASIGAMOS EL 2 (UNKNOWN)\n",
        "word2int[\"<PAD>\"] = 0\n",
        "word2int[\"<START>\"] = 1\n",
        "word2int[\"<UNK>\"] = 2\n",
        "word2int[\"<UNUSED>\"] = 3\n",
        "int2word = {i: w for w, i in word2int.items()} # LE HACEMOS LUEGO EL INVERSO PORQUE A CADA INDICE LE CORRESPONDE UNA PALABRA.\n",
        "num_words = num_words+3 # LE AÑADIMOS LAS 3 PALABARS ADICIONALES A NUESTRO VOCABUALRIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4SuQ_x8lSJr"
      },
      "source": [
        "For transforming an id-sequence to a phrase use get_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9TY23NslSJr"
      },
      "outputs": [],
      "source": [
        "# CREAMOS ESTA FUNCION QUE ES PARA TRANSFORMAR LA LISTA DE INDICES A PALABRAS. VAMOS A IR RECORRIENDO CADA INDICE DENTRO DE UNA FRASE Y LE APLICAMOS EL DICCIONARIO\n",
        "def get_words(sentence, int2word):\n",
        "    return ' '.join([int2word.get(i,'<UNK>') for i in sentence]) # SI NO ESTA LA PALABRA EN EL DICCIONARIO LE PONEMOS UNK.\n",
        "print(get_words(train_data[0], int2word))\n",
        "print('Sentiment: ', train_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llCo4TbylSJs"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "You need to convert your raw text to an appropriate input to a sequential model\n",
        "LUEGO CREAMOS LA FUNCIÓN INVERSA PARA PASAR UN TEXTO A INDICES. POR EJEMPLO EL TEXTO ANTERIOR LO VOLVEMOS A PASAR A INDICES\n",
        "ENTONCES CON ESTAS 2 FUNCIONES TENDREMOS QUE JUGAR LUEGO PARA HACER LAS PREDICCIONES."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqn7X6TTlSJs"
      },
      "outputs": [],
      "source": [
        "def vectorize_text_sentence(text, word2int):\n",
        "    tokens = text.split(' ')\n",
        "    tokens_id = [word2int.get(tk,2) for tk in tokens]\n",
        "    return tokens_id\n",
        "\n",
        "text = get_words(train_data[0], int2word)\n",
        "print(text)\n",
        "print(vectorize_text_sentence(text, word2int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_RYUKXTlSJs"
      },
      "source": [
        "### Bag of Words Model BoW\n",
        "\n",
        "HAY QUE TENER EN CUENTA QUE CADA DOCUMENTO TIENE UNA LONGITUD VARIABLE. HEMOS VISTO QUE A UNA RED NEURONAL HAY QUE DECIRLE LA DIMENSION DE ENTRADA. TENEMOS QUE USAR UNA DIMENSION FIJA. ENTONCES HABRA QUE VER UNA MANERA PARA PDER TRANSFORMAR TODOS LOS TEXTOS A UNA DIMENSION FIJA (VECTOR DE LA MISMA DIMENSION)\n",
        "VAMOS A USAR EL Bag of Words Model QUE ES UNA MANERA MUY TÍPICA DE REPRESENTAR LOS DOCUMENTOS. CONSISTE EN QUE TENEMOS UN TEXTO DE ENTRADA, LUEGO LO DIVIDIMOS EN PALABRAS Y CREAMOS UNA BAG OF WORDS (BOLSA DE PALABRAS) QUE ES CADA PALABRA Y EL NÚMERO DE VECES QUE APARECE. AQUI VEMOS QUE JOHN APARECE UNA VEZ PERO LIKES APARECE 2 VECES. ESTO TAMBIEN SE PUEDE HACER NORMALIZANDO ENTRE EL NÚMERO DE PALABRAS. POR EJEMPLO AQUI TENEMOS 9 PALABRAS (1+2+1+1+2+1+1) POR LO QUE 1/9=0.11, etc.\n",
        "EN ESTA MANERA DE REPRESNETAR UN DOCUMENTO, NO IMPORTA EL ORDEN. EL ORDEN SERÁ UN PROBLEMA PERO QUE GENERALMENTE VA A FUNCIONAR MUY BIEN.\n",
        "\n",
        "\n",
        "We are going to use a bag of words model. BoW is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the Each key is the word, and each value is the frequency of occurrences of that word in the given text document.\n",
        "\n",
        "- **Input document**: `\"John likes to watch movies Mary likes movies too\"`\n",
        "- **BoW**: `{'John': 1, 'likes': 2, 'to': 1, 'watch': 1, 'movies': 2, 'Mary': 1, 'too': 1}`\n",
        "- **BoW Normalized**: `{'John': 0.11, 'likes': 0.22, 'to': 0.11, 'watch': 0.11, 'movies': 0.22, 'Mary': 0.11, 'too': 0.11}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2H3swB9lSJs"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# ENTONCES CON ESTA FUNCION PARTIMMOS DE UNA SECUENCIA CON COUNTER QUE NOS DA EL NÚMERO DE OCURRENCIAS DE UNA LISTA\n",
        "def get_bag_of_words(sequence, norm=True):\n",
        "    word_count = Counter(sequence)\n",
        "    if norm: # SI LO QUEREMOS NORMALIZAR O NO.\n",
        "        total = sum(word_count.values())\n",
        "        word_freq = {w: n / total for w, n in word_count.items()}\n",
        "        return word_freq\n",
        "    else:\n",
        "        return dict(word_count.items())\n",
        "\n",
        "text_example = \"John likes to watch movies Mary likes movies too\"\n",
        "print('text_example', text_example)\n",
        "text_sequence = text_example.split()\n",
        "print('text splitted', text_sequence)\n",
        "bag_of_words = get_bag_of_words(text_sequence)\n",
        "print('bag_of_words', bag_of_words)\n",
        "print('bag_of_words norm=False', get_bag_of_words(text_sequence, norm=False))\n",
        "print(\n",
        "    'bag_of_words with indexes', {\n",
        "        word2int[w.lower()]: p\n",
        "        for w, p in get_bag_of_words(text_sequence, norm=False).items()\n",
        "    })\n",
        "\n",
        "# ESTO NOS DEVUELVE LA FRASE DEL EJEMPLO NORMALIZADA Y SIN NORMALIZAR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_quKlpExlSJs"
      },
      "source": [
        "### Document Term Matrix\n",
        "\n",
        "A APRTIR DE LO ANTERIOR CREAMOS UN DOCUMENT TERM MATRIX QUE CONSISTE EN CREAR UN VECTOR DE TAMAÑO IGUAL A NUESTRO VOCABULARIO. A CADA PALABRA LE CORRESPONDERÁ UNA CELDA DE NUESTRO VECTOR.\n",
        "We need a way to model the documents so that they are all the same length, so that we can use a neural network. For this we are going to use the document term matrix.\n",
        "- Every document is a vector with the dimension of the vocabulary.\n",
        "- The position i of the vector corresponds to the word with index i.\n",
        "- The vector is all zeros except for the BoW word positions, which are filled with the frequency of the corresponding word.\n",
        "\n",
        "\n",
        "POR EJEMPLO SI NEUSTRI BOCABULARIO SOLO FUERAN LAS SIGUIENTES 4 PALABRAS. DEFINIMOS QUE CADA DOCUMENTO VA A SER UN VECTOR DE 0 DE LA MISMA DIMENSION QUE EL VOCABUALRIO. LO QUE VA A TENER EN LA POSICIÓN DE CADA PALABRA ES POR EJEMPLO EL 'I' APARECE UNA VEZ  POR LO QUE LE PONEMOS 1. 'like' APARECE 1 VEZ Y LE PONEMOS 1. 'dislike' no aparece y 'movies' si por lo que ponemos 0 y 1.\n",
        "PARA TEXTOS MÁS LARGOS CADA DOCUEMNTO VA A ESTAR REPRESENTADO COMO UN VECTOR DE CEROS DEL TAMAÑO DE LA DIEMNSION DEL VOCABUALRIO Y CADA PALABRA/OCURRENCIA PONDREMOS EL BNUMERO QUE APARECE. ESTO TAMBIEN SE PODRÁ NORMALIZAR\n",
        "\n",
        "\n",
        "For example:\n",
        "\n",
        "- D1 = \"I like movies\",  `{'I': 1, 'like': 1, 'movies': 1}`\n",
        "\n",
        "- D2 = \"I dislike movies\",  `{'I': 1, 'dislike': 1, 'movies': 1}`\n",
        "\n",
        "Then the document-term matrix would be:\n",
        "\n",
        "\n",
        "\n",
        "   | Doc| I  | like    | dislike   | movies   |\n",
        "|---:|:-------------|:-----------|:------|:------|\n",
        "| D1 | 1  | 1       | 0   | 1     |\n",
        "| D2 | 1  | 0    | 1   | 1     |\n",
        "\n",
        "$D1 = [1,1,0,1]$\n",
        "\n",
        "$D2 = [1,0,1,1]$\n",
        "\n",
        "We convert every BoW to a vector of `dim=num_words` with `vectorize_sequences`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE_PB5z_lSJs"
      },
      "outputs": [],
      "source": [
        "# VAMOS A VECTORIZAR CON LAS SIGUIENTES FUNCIONES.\n",
        "# DECLARAMOS UN VECTOR DE CEROS IGUAL AL TAMAÑO DE NUESTRO VOCABUALARIO, COGEMOS EL BAG OF WORDS Y VAMOS RECORRIENDO LAS PALABRAS Y LA FRECUENCIA EN LA QUE APARECEN.\n",
        "# LO IREMOS PONIENDO EN LA POSICION DEL VECTOR vec[w] = freq\n",
        "\n",
        "def vectorize_sequence(sequence, num_words, norm=True):\n",
        "    vec = np.zeros(num_words)\n",
        "    bow = get_bag_of_words(sequence, norm)\n",
        "    for w, freq in bow.items():\n",
        "        if w < num_words:\n",
        "            vec[w] = freq\n",
        "    return vec\n",
        "\n",
        "# LUEGO ESTA FUNCIN CREA TODA LA MATRIZ DE TODO NUESTRO DATASET. CADA UNO DE LOS 25000 DOCUMENTOS DE TRAIN LO VAMOS A TRANSFORMAR A UN VECTOR DE DIMENSIÓN DEL TAMAÑO DEL VOCABULARIO\n",
        "# EN ESTE CASO SERÁN 10000 + LOS 3 INDICES= 13000.\n",
        "def vectorize_sequences(sequences, num_words=num_words, norm=True):\n",
        "    \"\"\"Creates an all-zero matrix of shape (len(sequences), num_words)\"\"\"\n",
        "    results = np.zeros((len(sequences), num_words))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, :] = vectorize_sequence(sequence, num_words, norm)\n",
        "    return results\n",
        "\n",
        "\n",
        "x_train = vectorize_sequences(train_data, num_words=num_words)\n",
        "x_test = vectorize_sequences(test_data, num_words=num_words)\n",
        "y_train =np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "x_train.shape, y_train.shape\n",
        "\n",
        "# TRAIN SERÁ UNA MATRIZ DE 25000,10003 QUE SON LOS 25000 DOCUMENTOS POR 10003 QUE ES EL NÚMERO DE PALABRAS. Y SERÁ 25000 CEROS Y UNOS.\n",
        "# A APRTIR DE AQUI YA TENEMOS UNA ENTRADA PARA LA RED NEURONAL Y A APRTIR DE AQUI VAMOS A DISEÑAR LA RED PARA PREDECIR EL SENTIMIENTO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7SfpXmelSJs"
      },
      "source": [
        "## Define and train a model\n",
        "\n",
        "ENTONCES AQUI LA PRÁCTI VA A SER DISEÑARLO COMO HEMOS VISTO EN CASOS ANTERIORES O CON LA API FUNCINAL O CON LA API SECUENCIAL.\n",
        "DISEÑAMOS UN MODELO QUE INTENTE CONSEGUIR 0.85 DE ACCURACY Y EN EL TEST INTENTAR LLEGAR A 0.9.\n",
        "PRIMERO HAY QUE COMENZAR SOLO CON UNA CAPA, VER UNAS NEURONAS POR CAPA  E IR SUBIENDOLO HASTA IR OBTENIENDO MEJORES RESULTADOS.\n",
        "\n",
        "\n",
        "\n",
        "Define, compile and fit your NN model\n",
        "\n",
        "1. You can use the [Functional API](https://keras.io/guides/functional_api/):\n",
        "\n",
        "You need to start with an input data entry:\n",
        "```python    \n",
        "    inputs = keras.Input(shape=(8,))\n",
        "    layer_1 = layers.Dense(...)(inputs)\n",
        "```\n",
        "\n",
        "and the network outputs:\n",
        "```python\n",
        "outputs = layers.Dense(...)(previous_layer)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "```\n",
        "\n",
        "2. Or you can use [Sequential API](https://keras.io/guides/sequential_model/)\n",
        "\n",
        "```python\n",
        "model = keras.Sequential(name='example_model')\n",
        "model.add(layers.Dense(..., input_shape=(8,))\n",
        "model.add(...\n",
        "```\n",
        "\n",
        "You can introduce regularization methods seen in [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb) like [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout):\n",
        "\n",
        "\n",
        "```python\n",
        "tf.keras.layers.Dropout(\n",
        "    rate, noise_shape=None, seed=None, **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "With Functional API:\n",
        "```python\n",
        "next_layer = layers.Dropout(0.4)(prev_layer)\n",
        "```\n",
        "With Sequential:\n",
        "```python\n",
        "model.add(layers.Dropout(0.4))\n",
        "```\n",
        "\n",
        "First try with only one hidden layer and see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoUFNekElSJt"
      },
      "outputs": [],
      "source": [
        "model = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "VPOUYHtjlSJt",
        "outputId": "57ea8622-1305-46fa-f8f2-51342e49457f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c4f3891bb80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_layer'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# entrada AQUI POR EJEMPLO VAMOS A PONER UNA SOLA CAPA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ml_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sigmoide'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# llama a las entradas. Le ponemos por ejemplo 4 unidades y la función de activación sigmoide. Ya vimos que normalmente la mejor es la relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# LA SALIDA COMO ES UNA CLASIFICACION BINARIA, TIENE QUE SER UNA DENSA CON UNA SIGMOIDE Y UNA SOLA NEURONA. PONEMOS LA SIGMOIDE PORQUE ES CLASIFICACION BIANRIA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sigmoide'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "inputs = tf.keras.Input(shape=(num_words,), name='input_layer')  # entrada AQUI POR EJEMPLO VAMOS A PONER UNA SOLA CAPA\n",
        "l_1 = layers.Dense(4,'sigmoide')(inputs) # llama a las entradas. Le ponemos por ejemplo 4 unidades y la función de activación sigmoide. Ya vimos que normalmente la mejor es la relu\n",
        "# LA SALIDA COMO ES UNA CLASIFICACION BINARIA, TIENE QUE SER UNA DENSA CON UNA SIGMOIDE Y UNA SOLA NEURONA. PONEMOS LA SIGMOIDE PORQUE ES CLASIFICACION BIANRIA\n",
        "outputs = layers.Dense(1,'sigmoide')(l_1)\n",
        "\n",
        "# Model definition\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# ENTONCES YA TENDRIAMOS EL MODELO, LO COMPILAMOS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AyU1xZ6lSJt"
      },
      "outputs": [],
      "source": [
        "es_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # can be 'val_accuracy'\n",
        "    patience=5,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
        "    restore_best_weights=True,\n",
        "    verbose=1)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "history = model.fit(x_train, y_train, validation_split=0.25, epochs= 25, batch_size=32, callbacks=[es_callback] # LO ENTRENAMOS SOLO CON  5 EPOCAS\n",
        "                    # ES BUENA IDEA AQUI USAR CALLBACK PARA APLICAR EL EARLY STOPPING. ASI LE PODEMOS PONER MÁS EPOCAS Y QUE PARE EN EL MOMENTO QUE QUERAMOS\n",
        ")\n",
        "\n",
        "# LO COMPILAMOS CON ADAM PARA IR MONITORIZANDO LOS RESULTADOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxQa-RE5lSJt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "def show_loss_accuracy_evolution(hist):\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Sparse Categorical Crossentropy')\n",
        "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
        "    ax1.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
        "    ax2.grid()\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "show_loss_accuracy_evolution(hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qb0XEkqlSJt"
      },
      "source": [
        "### Evaluate the model\n",
        "You need to obtain a Test Accuracy > 0.85. Try to get more than 0.9!\n",
        "\n",
        "VAMOS PROBANDO PORQUE EL OBJETIVO ES SUBIR DE 0.85 Y CASI LLEGAR A 0.9. ES BASTANTE SENCILLO LLEGAR CON PROBAR POCA COSA.\n",
        "\n",
        "COMO EL MODELO ANTERIOR NO HA SIDO SUFICIENTE PODEMOS PROBAR A SUBIR EL NÚMERO DE NEURONAS POR EJEMPLO A 8 Y PODEMOS PONER UNA CAPA MÁS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8WS4frBlSJt"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HqdzFldlSJt"
      },
      "outputs": [],
      "source": [
        "def show_errors(x_test, model, labels, int2word, n_samples=10):\n",
        "    preds = 1.0 * (model.predict(x_test).flatten() > 0.5)\n",
        "    bad_pred_inds = np.where(preds != labels)[0]\n",
        "    n_samples = min(len(bad_pred_inds), n_samples)\n",
        "    samples_inds = np.random.choice(bad_pred_inds, n_samples)\n",
        "    for ind in samples_inds:\n",
        "        print('Predicted : {0}, real : {1}, lenght: {2}'.format(\n",
        "            int(preds[ind]), labels[ind], len(test_data[ind])))\n",
        "        print(get_words(test_data[ind], int2word))\n",
        "        print()\n",
        "    return\n",
        "\n",
        "show_errors(x_test, model, y_test, int2word, n_samples=10)\n",
        "\n",
        "# CON ESTA FUNCION LUEGO PODMEOS ANALIZAR LOS RESULTADOS Y VER CUANDO HA FALLADO. ES ES PORQUE HABIA POCAS PALABRAS O POR QUE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBAMOS ALTERNATIVAS A VER SI MEJORA\n",
        "\n",
        "inputs = tf.keras.Input(shape=(num_words,), name='input_layer')\n",
        "l_1 = layers.Dense(8,'sigmoide')(inputs)\n",
        "l_2 = layers.Dense(8,'sigmoide')(l_1)\n",
        "outputs = layers.Dense(1,'sigmoide')(l_2)\n",
        "# ESTO YA SERÍA UN MODELO MÁS COMPLEJO. LE PODRÍAMOS PONER DROPOUT ENTRE MEDIAS:\n",
        "\n",
        "# Model definition\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    verbose=1)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "history = model.fit(x_train, y_train, validation_split=0.25, epochs= 5, batch_size=32, callbacks=[es_callback]\n",
        "\n",
        ")\n",
        "\n",
        "# VEMOS COMO VA MEJORANDO. PODRIAMOS SI NO PRPOBAR A PONER MÁS EPOCAS, MÁS NEURONAS...\n",
        "# EL OBEJTIVO ES QUE NOSOTROS VAYAMOS PROBANDO HAYA MEJORAR UN POCO EL MODELO.\n",
        "\n",
        "# CON LA SIGUIENTE FUNCIÓN PODEMOS VER CÓMO FUNCINARÍA EN DATOS NUEVOS\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ],
      "metadata": {
        "id": "vAg48dzgw1b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTwBkB5MlSJt"
      },
      "source": [
        "### Making predictioins with new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzg03i9xlSJt"
      },
      "outputs": [],
      "source": [
        "# HEMOS VISTO QUE EL MODELO BAG OF WORDS HACE QUE NO IMPORTE EL ORDEN\n",
        "# AQUI VEMOS PARA CADA TEXTO EL SENTIMIENTO QUE PREDICE. YA HEMOS VIST ¡O QUE BAGS OF WORDS ESTO NO LO VA A PREDECIR BIEN PORQUE NO DETECTA EL ORDEN\n",
        "# POR EJEMPLO LA FRASE the movie is not bad I like it ES IGUAL QUE SI PONDRIA the movie is bad I dont like it\n",
        "\n",
        "\n",
        "reviews = ['the film was really bad and i am very disappointed',\n",
        "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
        "           'this film was just brilliant',\n",
        "           'the movie is not bad',\n",
        "           'the movie is not bad I like it'\n",
        "]\n",
        "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
        "             for review in reviews]\n",
        "\n",
        "x_pred = vectorize_sequences(sequences, num_words=num_words)\n",
        "predictions = model.predict(x_pred)\n",
        "for review, pred in zip(reviews, predictions.flatten()):\n",
        "    print()\n",
        "    print(review)\n",
        "    print('Sentiment: ', np.round(pred, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntHMF2xmlSJt"
      },
      "outputs": [],
      "source": [
        "1.0*(model.predict(x_pred) > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGAU7e2ilSJt"
      },
      "source": [
        "### Question 1: Repeat the process with unnormalized bag of words, compare results\n",
        "PARA VER LA FEATURE ENGINEERING VAMOS A VER QUE PASA SI NO NORMALIZAMOS LOS BAG OF WORDS Y LO DEJAMOS COMO EL CONTEO QUÉ OCURRE.\n",
        "PODEMOS TENER UN TEXTO EN EL QUE LA PALABRA 'buena' APAREZCA UNA VEZ PERO SEA MUY CORTO. SIN EMBARGO TAMBIEN PUEDE HABER UN TEXTO QUE TENGA 10000 PALABRAS Y QUE LA PALABRA BUENA SOLO APAREZCA UNA VEZ.\n",
        "SI NORMALIZAMOS ESTAREMOS PONIENDOLO EN RELACION CON LA LONGITUD DEL DOCUMENTO. SI NO DARÍA IGUAL. LOS DOS TENDRÍAN EL MISMO VALOR. ENTONCES CON EL MODELO ANTERIOR, QUE HAYQUE VOLVER A EJECUATRLO CADA VEZ QUE LO QUERAMOS VOLVER A ENTRENAR PORQUE SI NO SE QUEDA CON LOS PESOS ANTERIORES.\n",
        "SI LO ENTRENAMOS, VEREMOS QUE A VECES TIENE PEOR RESULATDO SIN NORMALIZAR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-O0u_DWlSJt"
      },
      "outputs": [],
      "source": [
        "x_train = vectorize_sequences(train_data, num_words=num_words, norm=False)\n",
        "x_test = vectorize_sequences(test_data, num_words=num_words, norm=False)\n",
        "y_train =np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQXDD5JXlSJt"
      },
      "outputs": [],
      "source": [
        "model = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsO1GFDclSJt"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(x_train, y_train, validation_split=0.25, epochs= 10, batch_size=32)\n",
        "show_loss_accuracy_evolution(hist)\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jskvcCtelSJt"
      },
      "source": [
        "### Question 2: What happens if you reduce the size of the vocabulary `num_words`, compare results\n",
        "COMO PRÁCTICA NOS HA DEJADO REDUCIR EL NÚMERO DE VOCABUALRIO, AUMENTARLO Y COMPARAR LOS RESULTADOS PARA VER SI EL PROBLEMA ERA DE ESO.\n",
        "LUEGO COMO CURIOSIDAD PODEMOS COMPARARLO CON OTROS MODELOS DE ML Y VER QUE SI HEMOS HECHO UNA RED LO SUFICIENTEMENTE BUENA LOS VA MEJORANDO GENERALMENTE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaTdNLkplSJu"
      },
      "outputs": [],
      "source": [
        "num_words = ...\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "print(train_data[0])\n",
        "\n",
        "word2int = imdb.get_word_index()\n",
        "word2int = {w: i+3 for w, i in word2int.items()}\n",
        "word2int[\"<PAD>\"] = 0\n",
        "word2int[\"<START>\"] = 1\n",
        "word2int[\"<UNK>\"] = 2\n",
        "word2int[\"<UNUSED>\"] = 3\n",
        "int2word = {i: w for w, i in word2int.items()}\n",
        "num_words = num_words+3\n",
        "\n",
        "x_train = vectorize_sequences(train_data, num_words=num_words)\n",
        "x_test = vectorize_sequences(test_data, num_words=num_words)\n",
        "y_train =np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxMe4H2ZlSJu"
      },
      "outputs": [],
      "source": [
        "model = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPVq5lzklSJu"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(x_train, y_train, validation_split=0.25, epochs= 10, batch_size=32)\n",
        "show_loss_accuracy_evolution(hist)\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUIA9x58lSJu"
      },
      "outputs": [],
      "source": [
        "reviews = [\n",
        "    'the film was really bad and i am very disappointed',\n",
        "    'The film was very funny entertaining and good we had a great time . brilliant film',\n",
        "    'this film was just brilliant', 'the movie is not bad',\n",
        "    'the movie is not bad I like it'\n",
        "]\n",
        "sequences = [\n",
        "    vectorize_text_sentence(review.lower(), word2int) for review in reviews\n",
        "]\n",
        "\n",
        "x_pred = vectorize_sequences(sequences, num_words=num_words)\n",
        "predictions = model.predict(x_pred)\n",
        "for review, pred in zip(reviews, predictions.flatten()):\n",
        "    print()\n",
        "    print(review)\n",
        "    print('Sentiment: ', np.round(pred, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XvLzXF9lSJu"
      },
      "source": [
        "### Compare with other ML algorithms\n",
        "LUEGO COMO CURIOSIDAD PODEMOS COMPARARLO CON OTROS MODELOS DE ML Y VER QUE SI HEMOS HECHO UNA RED LO SUFICIENTEMENTE BUENA LOS VA MEJORANDO GENERALMENTE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygNRdzVnlSJu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clr = LogisticRegression()\n",
        "clr.fit(x_train, y_train)\n",
        "val_acc = clr.score(x_test, y_test)\n",
        "print('Test Accuracy: {}'.format(val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q8VgGg1lSJu"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(max_depth=5, n_jobs=-1)\n",
        "rfc.fit(x_train, y_train)\n",
        "val_acc = rfc.score(x_test, y_test)\n",
        "print('Test Accuracy: {}'.format(val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4sh_qp3lSJu"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_jobs=-1)\n",
        "rfc.fit(x_train, y_train)\n",
        "val_acc = rfc.score(x_test, y_test)\n",
        "print('Test Accuracy: {}'.format(val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBQLnkexlSJu"
      },
      "outputs": [],
      "source": [
        "reviews = []\n",
        "# training reviews\n",
        "for ind in range(len(train_data)):\n",
        "    sentence = train_data[ind]\n",
        "    sentence_text = get_words(train_data[ind], int2word)\n",
        "    reviews.append(sentence_text)\n",
        "print('First training review: ', reviews[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}