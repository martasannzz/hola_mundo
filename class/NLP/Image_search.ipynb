{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martasannzz/hola_mundo/blob/main/class/NLP/Image_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a687c92e",
      "metadata": {
        "id": "a687c92e"
      },
      "source": [
        "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
        "<table align=\"center\">\n",
        " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/NLP/Image_search.ipynb\">\n",
        "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/NLP/Image_search.ipynb\">\n",
        "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72febd9",
      "metadata": {
        "id": "e72febd9"
      },
      "source": [
        "# Introduction to Image Similarity\n",
        "\n",
        "In this notebook, we'll introduce image search using [`sentence-transformers`](https://www.sbert.net/), a Python library for state-of-the-art sentence, text and image embeddings.\n",
        "\n",
        "## What is Image Similarity?\n",
        "\n",
        "Image similarity refers to the process of finding images that are visually alike. This can range from finding near-identical duplicates to grouping images based on thematic resemblance. The implications of this technology are profound, as it underpins systems in:\n",
        "\n",
        "- **Visual Search:** Retailers and online marketplaces use image similarity to provide product recommendations based on user-uploaded photos. This technology enhances the shopping experience by allowing users to search for products using images instead of words.\n",
        "\n",
        "- **Content Discovery:** Social media platforms and content management systems rely on image similarity to categorize and recommend content, helping users discover new posts related to what they already like.\n",
        "\n",
        "- **Digital Archiving:** In libraries and archives, image similarity helps in organizing, indexing, and retrieving visual content from vast databases, making it easier to find historical documents and artworks.\n",
        "\n",
        "- **Security and Surveillance:** Image similarity algorithms can identify objects or persons of interest across different video frames or locations, contributing to safety and law enforcement efforts.\n",
        "\n",
        "- **Healthcare:** In medical imaging, similarity measures can help in identifying similar case histories, understanding disease progression, and even assisting in diagnosis by comparing patient scans with a database of known conditions.\n",
        "\n",
        "## Why is it Challenging?\n",
        "\n",
        "Images can vary in size, angle, lighting, and even be partially obscured. Traditional methods that rely on exact matches fall short in providing relevant results under these conditions. Hence, modern image similarity techniques employ deep learning, particularly leveraging models like CLIP (Contrastive Language-Image Pretraining) developed by OpenAI, to understand and quantify the likeness in a way that mimics human perception.\n",
        "\n",
        "## Objectives of this Tutorial\n",
        "\n",
        "In this tutorial, we will delve into how deep learning, especially through the use of Sentence Transformers and the CLIP model, enables us to map images and texts into a shared vector space. This mapping allows us to perform nuanced search and retrieval tasks, offering a bridge between textual descriptions and visual content.\n",
        "\n",
        "By the end of this session, you'll understand how to:\n",
        "- Utilize the CLIP model to create embeddings for images and text.\n",
        "- Implement an image search system that can find similar images based on text queries.\n",
        "- Explore practical applications and considerations in deploying image similarity models.\n",
        "\n",
        "PARA ESTA PRÁCTICA VAMOS A ISAR DE ESTA LIBRERÍA LA PARTE DE MODELOS PRE ENTRENADOS Y MAS CONCREAMNTE LOS MODELOS CLIP\n",
        "TAL Y COMO SE VE EN LA WEB EXISTEN MODELOS DE DIFERENTES TAMAÑOS Y PRECISIONES. TAMBIEN HAY UN MODELO MULTILINGÜE. SIMPLEMETE HABRA QUE COGER EL NOMBRE DEL MODELO QUE QUERAMOS VER VER COMO USARLO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8c43ec",
      "metadata": {
        "id": "6b8c43ec"
      },
      "source": [
        "# Image search\n",
        "\n",
        "In this notebook, we'll introduce image search using Sentence Transformers, by mapping images and texts into the same vector space. This enables us to perform search and retrieval tasks for images based on textual descriptions.\n",
        "\n",
        "VAMOS A USAR MODELOS CLIP. SON MODELOS DESARROLLADOS POR OPEN IA ¡. SOM MODELOS MULTIMODALES (PERMITEN PROCESAR DIFERENTES TIPOS DE DATOS) EN ESTE CASO, IMAGENES Y TEXTOS A LA VEZ.\n",
        "\n",
        "To achieve this, we'll utilize the [CLIP (Contrastive Language-Image Pretraining)](https://openai.com/research/clip) model, which is designed to learn a joint embedding space for both images and texts.\n",
        "\n",
        "Contrastive Language-Image Pretraining (CLIP) is an AI model developed by OpenAI. It is designed to learn from a wide range of tasks by leveraging the connection between natural language and images.\n",
        "\n",
        "1. Multimodal Learning: CLIP is a multimodal model that can understand both images and text. It is pretrained on a large dataset containing pairs of images and their associated text captions, learning to associate visual concepts with natural language.\n",
        "\n",
        "2. Contrastive Learning: CLIP learns by optimizing a contrastive objective. It is trained to recognize which image-caption pairs are correct among a set of negative examples. By learning to score the correct image-text pairs higher than incorrect ones, the model learns a useful representation for both modalities.\n",
        "\n",
        "3. Architecture: CLIP uses a Transformer-based architecture for processing text and a Vision Transformer or ResNet architecture for processing images. The image and text encoders are jointly trained, allowing the model to align both modalities in a shared embedding space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e44036",
      "metadata": {
        "id": "c7e44036"
      },
      "outputs": [],
      "source": [
        "# Install the sentence-transformers library\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985e3a8b",
      "metadata": {
        "id": "985e3a8b"
      },
      "outputs": [],
      "source": [
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "import pickle\n",
        "import zipfile\n",
        "import copy\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as IPImage\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa59f447",
      "metadata": {
        "id": "fa59f447"
      },
      "outputs": [],
      "source": [
        "# First, we load the respective CLIP model\n",
        "model_name = 'clip-ViT-B-32'\n",
        "model = SentenceTransformer(model_name) # CREAMOS EL MODELO SentenceTransformer CON EL NOMBRE DEL MODELO\n",
        "# ESTO SIMPLEMENTE DECARGA EL MODELO Y CUANTO MÁS GRANDE MAS TARDARÁ Y MÁS SI TRABAJAMOS EN LOCAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b22ec6",
      "metadata": {
        "id": "33b22ec6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import StringIO, BytesIO\n",
        "\n",
        "# VAMOS A HACER UNA FUNCION PARA QUE DADA UNA URL, DESCARGAR LA IMAGEN CON EL OBEJTO IMAGEN\n",
        "def get_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "300f0287",
      "metadata": {
        "id": "300f0287"
      },
      "source": [
        "For searching images, we need an image set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73382b00",
      "metadata": {
        "scrolled": true,
        "id": "73382b00"
      },
      "outputs": [],
      "source": [
        "img_url_path = 'https://github.com/ezponda/intro_deep_learning/raw/main/images/'\n",
        "img_urls = [\n",
        "    f'{img_url_path}eiffel_tower.jpeg',\n",
        "    f'{img_url_path}taj_mahal.jpeg',\n",
        "    f'{img_url_path}colosseum.jpeg',\n",
        "    f'{img_url_path}great_wall_of_china.jpeg',\n",
        "    f'{img_url_path}statue_of_liberty.jpeg',\n",
        "]\n",
        "images = [get_image_from_url(url) for url in img_urls]\n",
        "# EN EL OBJETO IMAGE TENEMOS LAS IMAGENES OBTENIDAS QUE NOS HEMOS DESCARGADO\n",
        "\n",
        "print('Sample images: ')\n",
        "for url, image in zip(img_urls, images):\n",
        "    print('_'*50)\n",
        "    print(f'url: {url}')\n",
        "    display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9b644a",
      "metadata": {
        "id": "6f9b644a"
      },
      "outputs": [],
      "source": [
        "# EL MODELO CLIP PERMITE PROCESAR IMAGENES Y TEXTOS EN EL MISMO ESPACIO\n",
        "# VAMOS A HACER UNA REPRESENTACIÓN VECTORIAL/EMBEDDING DE CUALQUIER OBJETO QUE USEMOS, YA SEA IMAGEN O TEXTO\n",
        "img_embeddings = model.encode(images, # SE PUEDE HACER TANTO LA LISTA DE IMAGENES COMO LA LISTA DE TEXTOS\n",
        "                       batch_size=128,\n",
        "                       convert_to_tensor=True,\n",
        "                       show_progress_bar=True)\n",
        "img_embeddings = img_embeddings.cpu()\n",
        "print(img_embeddings.shape)\n",
        "# ESTO LO QUE HACE, YA SEA UNA IMAGEN O UN TEXTO, LO PASA A UN VECTOR DE 512 DIMENSIONES.\n",
        "#ENTONCES TODO OBEJTO QUE LE METAMOS, SEA DE LA DIEMSNION QUE SEA, LO VA A PASAR A UN VECTOR DE 512 DIMENSIONES\n",
        "# ES POR ELLO QUE PODEMOS REPRESENTAR CONJUNTAMENTE IMAGENE SY TEXTOS PORQUE VAN A TENER LA MISMA REPRESENTACIÓN VECTORIAL EN EL MISMO ESPACIO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb617fb8",
      "metadata": {
        "id": "bb617fb8"
      },
      "source": [
        "Now, let's define a function to perform image search, given a query and a list of image embeddings.\n",
        "\n",
        "UNA DE LAS PRIMERAS APLICACIONES ES SI NOSOTROS QUEREMOS BUSCAR CON UNA QUERY A UN TEXTO EN UN DATASET DE IMAGENES.\n",
        "LO QUE VAMOS A HACER ES CODIFICAR LAS 5 IMAGENES Y DE ESA FORMA TENDREMOS 5 VECTORES DE 512 DIMENSIONES.\n",
        "ENTONCES YO CUANDO QUIERA HACER UNA QUERY COMO \"DAME UN EDIFICIO DE CHINA\" LO QUE SE HACE ES COGER LA QUERY QUE ES UN STRING Y LA VOY A CODIFICAR PASÁNDOLA AL MISMO VECTOR DE 512 DIMENSIONES. DE ESTA FORMA VAMOS A TENER EL VECTOR DE MI QUERY QUE VA A TENER 512 DIMENSIONES Y VAMOS A TENER 5 IMAGENES DONDE CADA UNA VA A TENER 512 DIMENSIONES.\n",
        "PARA RECOGER LAS IMAGENES MÁS SIMIALRES EN ESTE NUEVO ESPACIO, LO QUE SE VA A HACER ES COGER EL VECTOR QUE SEA MÁS PARECIDO AL VECTOR DE MI QUERY. SE PUEDE HACER CON LA DISTANCIA EUCLIDEA, CON LA COSINE SIMILARITY. AQUI USAREMOS COSINE SIMILARITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18e17ba",
      "metadata": {
        "id": "c18e17ba"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "\n",
        "def image_search(query: str, model: SentenceTransformer, img_embeddings: np.ndarray,\n",
        "                 images: List[Image.Image], top_k: int = 2) -> None:\n",
        "    \"\"\"Perform an image search given a text query.\n",
        "\n",
        "    This function computes the cosine similarity between the query embedding and the image embeddings,\n",
        "    retrieves the top_k images with the highest similarity, and displays them.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query as a text string.\n",
        "        model (SentenceTransformer): The SentenceTransformer model used to encode the text and images.\n",
        "        img_embeddings (np.ndarray): Precomputed embeddings for the images.\n",
        "        images (List[Image.Image]): A list of PIL Image objects.\n",
        "        top_k (int): The number of top results to display.\n",
        "\n",
        "    \"\"\"\n",
        "    # Encode the query to get the embeddings\n",
        "    query_embedding = model.encode([query])[0] # CODIFICAMOS LA QUERY\n",
        "\n",
        "    # Compute the cosine similarities between the query embedding and the image embeddings\n",
        "    similarities = cosine_similarity([query_embedding], img_embeddings)[0] # VAMOS A LOS 5 EMBEDDINGS DE IMAGENES QUE TENEMOS\n",
        "\n",
        "    # Get the indices of the top_k most similar images\n",
        "    top_k_indices = np.argsort(-similarities)[:top_k] # LE PEDIMOS QUE NS DEVUELVA LAS MÁS SIMILARES/CERCANAS USANDO COSING SIMILARITY O LA DISTANCIA EUCLIDEA...\n",
        "\n",
        "    # Print the input query for reference\n",
        "    print(f\"Input query: {query}\\n\")\n",
        "\n",
        "    # Display the top_k similar images along with their similarity scores\n",
        "    for index in top_k_indices:\n",
        "        print('_' * 50)\n",
        "        print(f\"Similarity Score: {similarities[index]:.4f}\")  # Improved readability with formatting\n",
        "        display(images[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf9c113",
      "metadata": {
        "id": "bdf9c113"
      },
      "outputs": [],
      "source": [
        "image_search('A building in Paris', model, img_embeddings, images, top_k=2)\n",
        "# SI HACEMOS IMAGE SEARCH Y PONEMOS UN EDIFICIO EN PARIS NOS DEVUELVE LA IMAGEN QUE APARECE MÁS SIMIALR JUNTO CON EL SCORE DE SIMILARIDAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7da09ac",
      "metadata": {
        "id": "c7da09ac"
      },
      "outputs": [],
      "source": [
        "image_search('Find me an image of a famous monument in India', model, img_embeddings, images, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07a8391",
      "metadata": {
        "scrolled": true,
        "id": "c07a8391"
      },
      "outputs": [],
      "source": [
        "image_search('A building in China', model, img_embeddings, images, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fdb5270",
      "metadata": {
        "id": "8fdb5270"
      },
      "source": [
        "## Unsplash subset dataset\n",
        "\n",
        "LO ANTERIOR SE PUEDE HACER CON OBJETOS MÁS GRANDES. POR EJEMPLO AQUI SE TIENE EL DAATASET Unsplash QUE ES UN DATASET PUBLICO QUE TIENE 25000 IMAGENES.\n",
        "\n",
        "[Unsplash](https://unsplash.com/data) is a collaborative image dataset openly shared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6420491b",
      "metadata": {
        "id": "6420491b"
      },
      "outputs": [],
      "source": [
        "# PAARA HACER BUSQUEDAS O BUSQUEDAS DE SIMILARIDAD EN ESTAS IMAGENES LO QUE HAY QUE HACER\n",
        "\n",
        "# Next, we get about 25k images from Unsplash\n",
        "img_folder = './photos/'\n",
        "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
        "    os.makedirs(img_folder, exist_ok=True)\n",
        "\n",
        "    photo_filename = 'unsplash-25k-photos.zip'\n",
        "    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n",
        "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename) # DE LA PROPIA PAGINA DE SENTTRANSFORMERS YA TENEMOS LOS EMBEDDINGS\n",
        "\n",
        "    #Extract all images\n",
        "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
        "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
        "            zf.extract(member, img_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58298e75",
      "metadata": {
        "id": "58298e75"
      },
      "outputs": [],
      "source": [
        "# Now, we need to compute the embeddings\n",
        "# To speed things up, we destribute pre-computed embeddings\n",
        "# Otherwise you can also encode the images yourself.\n",
        "# To encode an image, you can use the following code:\n",
        "# from PIL import Image\n",
        "# img_emb = model.encode(Image.open(filepath))\n",
        "def read_image_from_path(file_path):\n",
        "    img = Image.open(file_path)\n",
        "    return img\n",
        "\n",
        "use_precomputed_embeddings = True # YA TENEMOS CALCUALDOS LOS EMBEDDINGS ASI QUE NOS LOS DESCARGAMOS\n",
        "\n",
        "if use_precomputed_embeddings:\n",
        "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
        "    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n",
        "        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n",
        "        # DE ESTA FORMA TENEMOS LOS 25000 EMB DE NUETSRAS IMAGENES\n",
        "\n",
        "    with open(emb_filename, 'rb') as fIn:\n",
        "        img_names, img_embeddings = pickle.load(fIn)\n",
        "\n",
        "\n",
        "    print(\"Images:\", len(img_names))\n",
        "else:\n",
        "    img_names = list(glob.glob('photos/*.jpg'))[:5_000] # AQUI HEMOS REDUCIDO EL NUMERO DE IMAGENES SI NO TUVIERAMOS LOS ENB YA CALCULADOS\n",
        "    print(\"Images:\", len(img_names))\n",
        "    images = [read_image_from_path(img_name) for img_name in  img_names]\n",
        "    # LO UNICO QUE HABRIA QUE HACER ES COGER LAS 5000 IMAGENES Y MODIFICARLAS\n",
        "    img_embeddings = model.encode(images, batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
        "    img_embeddings = img_embeddings.cpu()\n",
        "# DE ESTA FORMA YA TENEMOS LOS 25000 VECTORES DE 512 DIMENSIONES\n",
        "# AQUI LO QUE HACEMOS ES YA TENERLOS PRE COMPUTADOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce3a523",
      "metadata": {
        "id": "cce3a523"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import copy\n",
        "\n",
        "# VAMOS A HACER EXACTAMENTE LO MISMO QUE ANTES. VAMOS A TENER UNA QUERY, LOS 25000 EMBEDDINGS Y LUEGO EL DIRECTORIO DE IMAGENES GUARDADAS PARA NO TENER TODAS CARGADAS EN MEMORIA.\n",
        "# TENEMOS EL PATH QUE NOS HEMOS DESCARGADO CON TODOS LOS NOOMBRES DE IMAGENES\n",
        "\n",
        "\n",
        "def image_search_from_path(query, model: SentenceTransformer, img_embeddings: np.ndarray,\n",
        "                           img_folder: str, img_names: List[str], top_k: int = 2) -> None:\n",
        "    \"\"\"Perform an image search for a given textual query within a set of images located in a specified folder.\n",
        "\n",
        "    Args:\n",
        "        query (str or Image.Image): The textual query for searching similar images.\n",
        "        model (SentenceTransformer): The SentenceTransformer model used for encoding.\n",
        "        img_embeddings (np.ndarray): The precomputed embeddings of the images.\n",
        "        img_folder (str): The folder where images are stored.\n",
        "        img_names (List[str]): The filenames of the images.\n",
        "        top_k (int): The number of top results to return.\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate the embedding for the query\n",
        "        query_embedding = model.encode([query])[0]\n",
        "        # Calculate the similarities between the query embedding and image embeddings\n",
        "        similarities = cosine_similarity([query_embedding], img_embeddings)[0]\n",
        "        # Identify the indexes of the top_k most similar images\n",
        "        indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
        "        indexes = indexes[np.argsort(-similarities[indexes])]\n",
        "\n",
        "        print(f\"Input query: {query}\\n\")\n",
        "        for index in indexes:\n",
        "            similarity_score = similarities[index]\n",
        "            image_name = img_names[index]\n",
        "            image_path = os.path.join(img_folder, image_name) # CREAMOS EL PATH Y NOS DESCARGAMOS LA IMAGEN PARA PODER HACER UN PLOT\n",
        "            try:\n",
        "                with Image.open(image_path) as img:\n",
        "                    print('_' * 50)\n",
        "                    print(f\"Similarity: {similarity_score:.4f}\")\n",
        "                    display(copy.deepcopy(img))\n",
        "            except Exception as e:\n",
        "                print(f\"Error displaying image {image_name}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image search: {e}\")\n",
        "\n",
        "# ESTO NOS VA DANDO PARA CADA QUERY LAS IMAGENES MÁS SIMMIALRES PARA LO QUE PEDIMOS DE DENTRO DE LAS 25000 IMAGENES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4efc717",
      "metadata": {
        "id": "e4efc717"
      },
      "outputs": [],
      "source": [
        "image_search_from_path('A building in Paris', model, img_embeddings, img_folder, img_names, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833f2fb3",
      "metadata": {
        "id": "833f2fb3"
      },
      "outputs": [],
      "source": [
        "image_search_from_path('A building in China', model, img_embeddings, img_folder, img_names, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4c1443",
      "metadata": {
        "id": "bc4c1443"
      },
      "outputs": [],
      "source": [
        "image_search_from_path('A building in China', model, img_embeddings, img_folder, img_names, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35453318",
      "metadata": {
        "id": "35453318"
      },
      "outputs": [],
      "source": [
        "image_search_from_path('Two dogs playing in the snow', model, img_embeddings, img_folder, img_names, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fee3925",
      "metadata": {
        "id": "5fee3925"
      },
      "source": [
        "## Image-to-Image Search\n",
        "You can use the method also for image-to-image search.\n",
        "\n",
        "COMO ESTAMOS EN UN ESPACIO VECTORIAL, DIRECTAMENTE PODRÍAMOS DECIR QUE NOS DE LA IMAGEN MÁS CERCANA A X IMAGEN\n",
        "\n",
        "To achieve this, you pass `get_image_from_url(url)` to the search method.\n",
        "\n",
        "It will then return similar images\n",
        "\n",
        "POR EJEMPLO COGEMOS UNA IMAGEN DE LA TORRE EIFEL, SE LA PASAMOS Y QUE NOS DE LAS IMAGENES MÁS SIMILARES A ESA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f3b873c",
      "metadata": {
        "id": "0f3b873c"
      },
      "outputs": [],
      "source": [
        "img = get_image_from_url(img_urls[0])\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886bf68c",
      "metadata": {
        "id": "886bf68c"
      },
      "outputs": [],
      "source": [
        "image_search_from_path(img, model, img_embeddings, img_folder, img_names, top_k=5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}